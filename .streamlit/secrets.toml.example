# Streamlit Secrets Configuration Template
# Copy this file to .streamlit/secrets.toml and fill in your actual values
# Never commit the actual secrets.toml file to version control!

# ========================================
# OpenAI Configuration
# ========================================
# Uncomment and add your OpenAI API key
# OPENAI_API_KEY = "sk-proj-your-openai-api-key-here"

# ========================================
# OpenRouter Configuration
# ========================================
# Uncomment and add your OpenRouter API key
# OPENROUTER_API_KEY = "sk-or-your-openrouter-api-key-here"

# ========================================
# Ollama Configuration (Local Endpoint)
# ========================================
# Uncomment and configure your local Ollama endpoint
# OLLAMA_ENDPOINT_URL = "http://localhost:11434/v1"

# ========================================
# Usage Instructions
# ========================================
# 1. Copy this file: cp .streamlit/secrets.toml.example .streamlit/secrets.toml
# 2. Uncomment the configuration you want to use (OpenAI OR Ollama)
# 3. Replace the placeholder values with your actual credentials
# 4. Save the file and restart your Streamlit app
#
# Note: You only need to configure ONE backend (OpenAI OR Ollama)
# The app will automatically detect which one is available and use it.

# ========================================
# Example Configurations
# ========================================

# For OpenAI (recommended for production):
# OPENAI_API_KEY = "sk-proj-1234567890abcdef1234567890abcdef12345678"

# For OpenRouter (alternative with many models):
# OPENROUTER_API_KEY = "sk-or-1234567890abcdef1234567890abcdef12345678"

# For Ollama (recommended for local development):
# OLLAMA_ENDPOINT_URL = "http://localhost:11434/v1"
# Make sure Ollama is running: ollama serve
# Install a model: ollama pull llama2
